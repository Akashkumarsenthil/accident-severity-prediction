{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1199a21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "US ACCIDENTS SEVERITY PREDICTION - OPTIMIZED PIPELINE (67K per class)\n",
      "================================================================================\n",
      "\n",
      "[PART 1] RETRIEVING DATA FROM KAGGLE\n",
      "--------------------------------------------------------------------------------\n",
      "Downloading dataset from Kaggle...\n",
      "‚úì Dataset loaded: 7,728,394 rows √ó 46 columns\n",
      "‚úì Memory usage: 10870.28 MB\n",
      "\n",
      "[PART 2] INITIAL DATA EXPLORATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Dataset Info:\n",
      "  Shape: (7728394, 46)\n",
      "  Columns: 46\n",
      "\n",
      "Severity Distribution (Original):\n",
      "  Severity 1: 67,366 (0.87%)\n",
      "  Severity 2: 6,156,981 (79.67%)\n",
      "  Severity 3: 1,299,337 (16.81%)\n",
      "  Severity 4: 204,710 (2.65%)\n",
      "\n",
      "[PART 3] FEATURE ENGINEERING\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Creating time-based features...\n",
      "‚úì Time features created\n",
      "\n",
      "Creating weather flags...\n",
      "‚úì Weather flags created\n",
      "\n",
      "[PART 4] FEATURE SELECTION & DATA CLEANING\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Total features selected: 33\n",
      "\n",
      "[PART 4B] HANDLING MISSING VALUES STRATEGICALLY\n",
      "--------------------------------------------------------------------------------\n",
      "Initial dataset size: 7,728,394 rows\n",
      "\n",
      "Missing values per feature:\n",
      "  Temperature(F): 163,853 (2.1%)\n",
      "  Humidity(%): 174,144 (2.3%)\n",
      "  Pressure(in): 140,679 (1.8%)\n",
      "  Visibility(mi): 177,098 (2.3%)\n",
      "  Wind_Speed(mph): 571,233 (7.4%)\n",
      "  Precipitation(in): 2,203,586 (28.5%)\n",
      "  hour: 743,166 (9.6%)\n",
      "  dow: 743,166 (9.6%)\n",
      "  month: 743,166 (9.6%)\n",
      "  year: 743,166 (9.6%)\n",
      "  day: 743,166 (9.6%)\n",
      "\n",
      "üìä Strategy: Impute missing values instead of dropping rows\n",
      "--------------------------------------------------------------------------------\n",
      "Imputing numeric features with median...\n",
      "\n",
      "‚úì After imputation: 7,728,394 rows preserved\n",
      "\n",
      "Class distribution after data cleaning:\n",
      "  Severity 1.0: 67,366 (0.87%)\n",
      "  Severity 2.0: 6,156,981 (79.67%)\n",
      "  Severity 3.0: 1,299,337 (16.81%)\n",
      "  Severity 4.0: 204,710 (2.65%)\n",
      "\n",
      "[PART 5] HANDLING CLASS IMBALANCE - OPTIMIZED SAMPLING\n",
      "================================================================================\n",
      "\n",
      "‚ö†Ô∏è  CURRENT Distribution (IMBALANCED):\n",
      "--------------------------------------------------------------------------------\n",
      "  Severity 1.0:     67,366 ( 0.87%) \n",
      "  Severity 2.0:  6,156,981 (79.67%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Severity 3.0:  1,299,337 (16.81%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Severity 4.0:    204,710 ( 2.65%) ‚ñà\n",
      "\n",
      "üìä Class Distribution Analysis:\n",
      "  Smallest class: Severity 1.0 with 67,366 samples\n",
      "  Largest class: Severity 2.0 with 6,156,981 samples\n",
      "  Imbalance ratio: 91:1\n",
      "\n",
      "================================================================================\n",
      "SAMPLING STRATEGY: Hybrid SMOTE + Undersampling (Target: 67K per class)\n",
      "================================================================================\n",
      "\n",
      "üéØ Target: 67,000 samples per class\n",
      "   Total after balancing: 268,000 samples\n",
      "\n",
      "Applying balanced resampling (with replacement for minorities)...\n",
      "--------------------------------------------------------------------------------\n",
      "Severity 1.0: Undersampling     67,366 ‚Üí 67,000\n",
      "Severity 2.0: Undersampling  6,156,981 ‚Üí 67,000\n",
      "Severity 3.0: Undersampling  1,299,337 ‚Üí 67,000\n",
      "Severity 4.0: Undersampling    204,710 ‚Üí 67,000\n",
      "\n",
      "================================================================================\n",
      "‚úÖ BALANCED Distribution (AFTER SAMPLING):\n",
      "================================================================================\n",
      "  Severity 1.0:     67,000 ( 25.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Severity 2.0:     67,000 ( 25.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Severity 3.0:     67,000 ( 25.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  Severity 4.0:     67,000 ( 25.0%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "üìä Summary:\n",
      "  Total samples: 268,000\n",
      "  From original: 7,728,394\n",
      "  Target per class: 67,000\n",
      "  Perfect balance achieved: All classes at 25.0%\n",
      "\n",
      "[PART 6] PREPARING MULTI-CLASS CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "Dataset prepared:\n",
      "  Features: 33\n",
      "  Samples: 268,000\n",
      "  Classes: [1. 2. 3. 4.]\n",
      "\n",
      "Train-test split:\n",
      "  Training: 214,400 samples (80%)\n",
      "  Testing:  53,600 samples (20%)\n",
      "‚úì Data scaled and ready for training\n",
      "\n",
      "[PART 7] PREPARING BINARY CLASSIFICATION\n",
      "================================================================================\n",
      "\n",
      "Binary mapping:\n",
      "  Severity 1, 2 ‚Üí LOW  (0)\n",
      "  Severity 3, 4 ‚Üí HIGH (1)\n",
      "\n",
      "Binary distribution:\n",
      "  LOW  (0): 134,000 (50.0%)\n",
      "  HIGH (1): 134,000 (50.0%)\n",
      "\n",
      "Binary split:\n",
      "  Training: 214,400 samples\n",
      "  Testing:  53,600 samples\n",
      "‚úì Binary data scaled and ready\n",
      "\n",
      "[PART 8] TRAINING MULTI-CLASS MODELS\n",
      "================================================================================\n",
      "‚ö†Ô∏è  XGBoost not available\n",
      "\n",
      "Training 3 models on 214,400 samples...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Logistic Regression\n",
      "  ----------------------------------------\n",
      "  Accuracy:  0.5289\n",
      "  Precision: 0.5166\n",
      "  Recall:    0.5289\n",
      "  F1-Score:  0.5160\n",
      "  Time:      4.30s\n",
      "\n",
      "Random Forest\n",
      "  ----------------------------------------\n",
      "  Accuracy:  0.7304\n",
      "  Precision: 0.7283\n",
      "  Recall:    0.7304\n",
      "  F1-Score:  0.7246\n",
      "  Time:      7.97s\n",
      "\n",
      "LightGBM\n",
      "  ----------------------------------------\n",
      "  Accuracy:  0.7318\n",
      "  Precision: 0.7285\n",
      "  Recall:    0.7318\n",
      "  F1-Score:  0.7267\n",
      "  Time:      3.17s\n",
      "\n",
      "================================================================================\n",
      "‚úÖ MULTI-CLASS TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "[PART 9] TRAINING BINARY CLASSIFICATION MODELS\n",
      "================================================================================\n",
      "\n",
      "Training 3 models...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Logistic Regression\n",
      "  ----------------------------------------\n",
      "  Accuracy:  0.7067\n",
      "  Precision: 0.6988\n",
      "  Recall:    0.7263\n",
      "  F1-Score:  0.7123\n",
      "  AUC-ROC:   0.7897\n",
      "  Time:      1.55s\n",
      "\n",
      "Random Forest\n",
      "  ----------------------------------------\n",
      "  Accuracy:  0.7908\n",
      "  Precision: 0.7694\n",
      "  Recall:    0.8304\n",
      "  F1-Score:  0.7988\n",
      "  AUC-ROC:   0.8730\n",
      "  Time:      6.89s\n",
      "\n",
      "LightGBM\n",
      "  ----------------------------------------\n",
      "  Accuracy:  0.7883\n",
      "  Precision: 0.7696\n",
      "  Recall:    0.8230\n",
      "  F1-Score:  0.7954\n",
      "  AUC-ROC:   0.8708\n",
      "  Time:      0.92s\n",
      "\n",
      "================================================================================\n",
      "‚úÖ BINARY CLASSIFICATION TRAINING COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "[PART 10] FINAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "üèÜ MULTI-CLASS MODEL RANKINGS (by Accuracy):\n",
      "--------------------------------------------------------------------------------\n",
      "ü•á 1. LightGBM                  ‚Üí Acc: 0.7318 | F1: 0.7267 | Time: 3.2s\n",
      "ü•à 2. Random Forest             ‚Üí Acc: 0.7304 | F1: 0.7246 | Time: 8.0s\n",
      "ü•â 3. Logistic Regression       ‚Üí Acc: 0.5289 | F1: 0.5160 | Time: 4.3s\n",
      "\n",
      "üèÜ BINARY MODEL RANKINGS (by AUC-ROC):\n",
      "--------------------------------------------------------------------------------\n",
      "ü•á 1. Random Forest             ‚Üí AUC: 0.8730 | F1: 0.7988 | Time: 6.9s\n",
      "ü•à 2. LightGBM                  ‚Üí AUC: 0.8708 | F1: 0.7954 | Time: 0.9s\n",
      "ü•â 3. Logistic Regression       ‚Üí AUC: 0.7897 | F1: 0.7123 | Time: 1.5s\n",
      "\n",
      "================================================================================\n",
      "üéâ TRAINING COMPLETE - OPTIMIZED WITH 268K SAMPLES (67K per class)!\n",
      "================================================================================\n",
      "\n",
      "üìà Key Improvements in this version:\n",
      "  ‚úì Target: 67,000 samples per class (268,000 total)\n",
      "  ‚úì Missing values imputed (not dropped) to preserve minority class samples\n",
      "  ‚úì SMOTE used for synthetic oversampling of minority classes\n",
      "  ‚úì Better representation of rare severity levels (1 and 4)\n",
      "  ‚úì More robust model performance and generalization\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# US ACCIDENTS SEVERITY PREDICTION - OPTIMIZED VERSION\n",
    "# =============================================================================\n",
    "# Uses optimal sampling: 67K per class (268K total) with SMOTE for minorities\n",
    "# FIXED: Now properly handles NaN values BEFORE determining sample sizes\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"US ACCIDENTS SEVERITY PREDICTION - OPTIMIZED PIPELINE (67K per class)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# PART 1: DATA RETRIEVAL FROM KAGGLE\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 1] RETRIEVING DATA FROM KAGGLE\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "# Download the dataset\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "df = kagglehub.load_dataset(\n",
    "    KaggleDatasetAdapter.PANDAS,\n",
    "    \"sobhanmoosavi/us-accidents\",\n",
    "    \"US_Accidents_March23.csv\"\n",
    ")\n",
    "\n",
    "print(f\"‚úì Dataset loaded: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"‚úì Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 2: INITIAL DATA EXPLORATION\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 2] INITIAL DATA EXPLORATION\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Columns: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\nSeverity Distribution (Original):\")\n",
    "severity_dist = df['Severity'].value_counts().sort_index()\n",
    "total = len(df)\n",
    "for sev, count in severity_dist.items():\n",
    "    pct = count / total * 100\n",
    "    print(f\"  Severity {sev}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 3: FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 3] FEATURE ENGINEERING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nCreating time-based features...\")\n",
    "\n",
    "# Convert datetime columns\n",
    "df['Start_Time'] = pd.to_datetime(df['Start_Time'], errors='coerce')\n",
    "\n",
    "# Extract time features\n",
    "df['year'] = df['Start_Time'].dt.year\n",
    "df['month'] = df['Start_Time'].dt.month\n",
    "df['day'] = df['Start_Time'].dt.day\n",
    "df['hour'] = df['Start_Time'].dt.hour\n",
    "df['dow'] = df['Start_Time'].dt.dayofweek\n",
    "\n",
    "# Create time-based flags\n",
    "df['is_weekend'] = (df['dow'] >= 5).astype(int)\n",
    "df['is_morning_peak'] = ((df['hour'] >= 6) & (df['hour'] <= 9)).astype(int)\n",
    "df['is_evening_peak'] = ((df['hour'] >= 16) & (df['hour'] <= 19)).astype(int)\n",
    "\n",
    "print(\"‚úì Time features created\")\n",
    "\n",
    "# Create weather condition flags\n",
    "print(\"\\nCreating weather flags...\")\n",
    "\n",
    "def create_weather_flags(df):\n",
    "    \"\"\"Create binary flags for weather conditions\"\"\"\n",
    "    if 'Weather_Condition' not in df.columns:\n",
    "        return df\n",
    "    \n",
    "    weather = df['Weather_Condition'].fillna('').str.lower()\n",
    "    \n",
    "    df['is_rain'] = weather.str.contains('rain|drizzle|shower', regex=True).astype(int)\n",
    "    df['is_snow'] = weather.str.contains('snow|sleet|ice|wintry', regex=True).astype(int)\n",
    "    df['is_fog'] = weather.str.contains('fog|mist|haze', regex=True).astype(int)\n",
    "    df['is_thunder'] = weather.str.contains('thunder|t-storm|storm', regex=True).astype(int)\n",
    "    df['is_wind'] = weather.str.contains('wind', regex=True).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = create_weather_flags(df)\n",
    "\n",
    "# Check if it's nighttime\n",
    "if 'Sunrise_Sunset' in df.columns:\n",
    "    df['is_night'] = (df['Sunrise_Sunset'] == 'Night').astype(int)\n",
    "else:\n",
    "    df['is_night'] = ((df['hour'] < 6) | (df['hour'] >= 20)).astype(int)\n",
    "\n",
    "print(\"‚úì Weather flags created\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 4: FEATURE SELECTION & DATA CLEANING\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 4] FEATURE SELECTION & DATA CLEANING\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Define feature groups\n",
    "numeric_features = [\n",
    "    'Temperature(F)', 'Humidity(%)', 'Pressure(in)', \n",
    "    'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)', \n",
    "    'Distance(mi)'\n",
    "]\n",
    "\n",
    "boolean_features = [\n",
    "    'Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', \n",
    "    'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', \n",
    "    'Traffic_Calming', 'Traffic_Signal'\n",
    "]\n",
    "\n",
    "time_features = [\n",
    "    'hour', 'dow', 'month', 'year', 'day', \n",
    "    'is_weekend', 'is_morning_peak', 'is_evening_peak'\n",
    "]\n",
    "\n",
    "weather_flags = [\n",
    "    'is_rain', 'is_snow', 'is_fog', 'is_thunder', 'is_wind', 'is_night'\n",
    "]\n",
    "\n",
    "# Combine all features\n",
    "all_features = []\n",
    "for feature in numeric_features + boolean_features + time_features + weather_flags:\n",
    "    if feature in df.columns:\n",
    "        all_features.append(feature)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Feature '{feature}' not found\")\n",
    "\n",
    "print(f\"\\nTotal features selected: {len(all_features)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 4B: HANDLE MISSING VALUES STRATEGICALLY (NEW!)\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 4B] HANDLING MISSING VALUES STRATEGICALLY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create ML dataframe\n",
    "ml_df = df[all_features + ['Severity']].copy()\n",
    "\n",
    "print(f\"Initial dataset size: {len(ml_df):,} rows\")\n",
    "\n",
    "# Check missing values per column\n",
    "print(\"\\nMissing values per feature:\")\n",
    "missing_counts = ml_df.isnull().sum()\n",
    "for col, count in missing_counts[missing_counts > 0].items():\n",
    "    pct = count / len(ml_df) * 100\n",
    "    print(f\"  {col}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# STRATEGY: Fill numeric features with median instead of dropping rows\n",
    "# This preserves many more samples, especially for minority classes\n",
    "print(\"\\nüìä Strategy: Impute missing values instead of dropping rows\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Separate numeric and non-numeric features for imputation\n",
    "numeric_cols = ml_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Impute numeric columns with median\n",
    "print(\"Imputing numeric features with median...\")\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "ml_df[numeric_cols] = numeric_imputer.fit_transform(ml_df[numeric_cols])\n",
    "\n",
    "# Check if any rows still have NaN (shouldn't be many)\n",
    "remaining_nan = ml_df.isnull().sum().sum()\n",
    "if remaining_nan > 0:\n",
    "    print(f\"Dropping {ml_df.isnull().any(axis=1).sum():,} rows with remaining NaN values\")\n",
    "    ml_df = ml_df.dropna()\n",
    "\n",
    "print(f\"\\n‚úì After imputation: {len(ml_df):,} rows preserved\")\n",
    "\n",
    "# Show class distribution AFTER cleaning\n",
    "print(\"\\nClass distribution after data cleaning:\")\n",
    "for sev, count in ml_df['Severity'].value_counts().sort_index().items():\n",
    "    pct = count / len(ml_df) * 100\n",
    "    print(f\"  Severity {sev}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 5: HANDLE CLASS IMBALANCE - OPTIMIZED SAMPLING (67K per class)\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 5] HANDLING CLASS IMBALANCE - OPTIMIZED SAMPLING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  CURRENT Distribution (IMBALANCED):\")\n",
    "print(\"-\" * 80)\n",
    "for sev, count in ml_df['Severity'].value_counts().sort_index().items():\n",
    "    pct = count / len(ml_df) * 100\n",
    "    bar = '‚ñà' * min(int(pct / 2), 50)\n",
    "    print(f\"  Severity {sev}: {count:>10,} ({pct:>5.2f}%) {bar}\")\n",
    "\n",
    "# Analyze class sizes\n",
    "class_counts = ml_df['Severity'].value_counts().sort_index()\n",
    "min_class = class_counts.min()\n",
    "min_severity = class_counts.idxmin()\n",
    "\n",
    "print(f\"\\nüìä Class Distribution Analysis:\")\n",
    "print(f\"  Smallest class: Severity {min_severity} with {min_class:,} samples\")\n",
    "print(f\"  Largest class: Severity {class_counts.idxmax()} with {class_counts.max():,} samples\")\n",
    "print(f\"  Imbalance ratio: {class_counts.max() / min_class:.0f}:1\")\n",
    "\n",
    "# =============================================================================\n",
    "# STRATEGY: Target 67K per class using SMOTE for minorities\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLING STRATEGY: Hybrid SMOTE + Undersampling (Target: 67K per class)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Target: 67,000 samples per class (268K total) - based on smallest original class\n",
    "TARGET_PER_CLASS = 67000\n",
    "\n",
    "print(f\"\\nüéØ Target: {TARGET_PER_CLASS:,} samples per class\")\n",
    "print(f\"   Total after balancing: {TARGET_PER_CLASS * 4:,} samples\")\n",
    "\n",
    "# Check if smallest class can support the target\n",
    "if min_class < TARGET_PER_CLASS:\n",
    "    print(f\"\\n‚ö†Ô∏è  Smallest class ({min_class:,}) is smaller than target ({TARGET_PER_CLASS:,})\")\n",
    "    print(\"   Will use SMOTE to generate synthetic samples for minority classes\")\n",
    "    USE_SMOTE = True\n",
    "else:\n",
    "    USE_SMOTE = False\n",
    "\n",
    "# Prepare features and target for SMOTE\n",
    "X_for_sampling = ml_df.drop('Severity', axis=1)\n",
    "y_for_sampling = ml_df['Severity']\n",
    "\n",
    "if USE_SMOTE:\n",
    "    print(\"\\nApplying SMOTE + RandomUnderSampler...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    try:\n",
    "        from imblearn.over_sampling import SMOTE\n",
    "        from imblearn.under_sampling import RandomUnderSampler\n",
    "        from imblearn.pipeline import Pipeline\n",
    "        \n",
    "        # Calculate sampling strategy\n",
    "        # For classes smaller than target: oversample to target\n",
    "        # For classes larger than target: undersample to target\n",
    "        over_sampling_strategy = {}\n",
    "        under_sampling_strategy = {}\n",
    "        \n",
    "        for severity in sorted(class_counts.index):\n",
    "            current_count = class_counts[severity]\n",
    "            if current_count < TARGET_PER_CLASS:\n",
    "                # Need to oversample this class\n",
    "                over_sampling_strategy[severity] = TARGET_PER_CLASS\n",
    "                print(f\"  Severity {severity}: SMOTE {current_count:,} ‚Üí {TARGET_PER_CLASS:,}\")\n",
    "            else:\n",
    "                # Will undersample later\n",
    "                under_sampling_strategy[severity] = TARGET_PER_CLASS\n",
    "                print(f\"  Severity {severity}: Undersample {current_count:,} ‚Üí {TARGET_PER_CLASS:,}\")\n",
    "        \n",
    "        # Create pipeline: SMOTE first (for minorities), then undersample (for majorities)\n",
    "        steps = []\n",
    "        \n",
    "        if over_sampling_strategy:\n",
    "            # SMOTE with k_neighbors adjusted for smallest class\n",
    "            min_samples = min(class_counts.values)\n",
    "            k_neighbors = min(5, min_samples - 1) if min_samples > 1 else 1\n",
    "            steps.append(('smote', SMOTE(\n",
    "                sampling_strategy=over_sampling_strategy,\n",
    "                random_state=42,\n",
    "                k_neighbors=k_neighbors,\n",
    "                n_jobs=-1\n",
    "            )))\n",
    "        \n",
    "        if under_sampling_strategy:\n",
    "            steps.append(('undersample', RandomUnderSampler(\n",
    "                sampling_strategy=under_sampling_strategy,\n",
    "                random_state=42\n",
    "            )))\n",
    "        \n",
    "        resampling_pipeline = Pipeline(steps=steps)\n",
    "        \n",
    "        print(\"\\nExecuting resampling pipeline...\")\n",
    "        X_resampled, y_resampled = resampling_pipeline.fit_resample(X_for_sampling, y_for_sampling)\n",
    "        \n",
    "        # Create balanced dataframe\n",
    "        ml_df_balanced = pd.DataFrame(X_resampled, columns=X_for_sampling.columns)\n",
    "        ml_df_balanced['Severity'] = y_resampled\n",
    "        \n",
    "        print(\"‚úì SMOTE resampling complete!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"\\n‚ö†Ô∏è  imbalanced-learn not installed. Using simple resampling with replacement...\")\n",
    "        print(\"   Install with: pip install imbalanced-learn\")\n",
    "        USE_SMOTE = False\n",
    "\n",
    "if not USE_SMOTE:\n",
    "    # Fallback: Simple resampling\n",
    "    from sklearn.utils import resample\n",
    "    \n",
    "    print(\"\\nApplying balanced resampling (with replacement for minorities)...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    \n",
    "    for severity in sorted(ml_df['Severity'].unique()):\n",
    "        severity_df = ml_df[ml_df['Severity'] == severity]\n",
    "        current_size = len(severity_df)\n",
    "        \n",
    "        if current_size >= TARGET_PER_CLASS:\n",
    "            # Undersample (random selection without replacement)\n",
    "            print(f\"Severity {severity}: Undersampling {current_size:>10,} ‚Üí {TARGET_PER_CLASS:,}\")\n",
    "            sampled = resample(\n",
    "                severity_df, \n",
    "                n_samples=TARGET_PER_CLASS, \n",
    "                random_state=42, \n",
    "                replace=False\n",
    "            )\n",
    "        else:\n",
    "            # Oversample (with replacement to reach target)\n",
    "            oversample_factor = TARGET_PER_CLASS / current_size\n",
    "            print(f\"Severity {severity}: Oversampling  {current_size:>10,} ‚Üí {TARGET_PER_CLASS:,} ({oversample_factor:.1f}x)\")\n",
    "            sampled = resample(\n",
    "                severity_df, \n",
    "                n_samples=TARGET_PER_CLASS, \n",
    "                random_state=42, \n",
    "                replace=True\n",
    "            )\n",
    "        \n",
    "        balanced_dfs.append(sampled)\n",
    "    \n",
    "    # Combine and shuffle\n",
    "    ml_df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "ml_df_balanced = ml_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ BALANCED Distribution (AFTER SAMPLING):\")\n",
    "print(\"=\" * 80)\n",
    "for sev, count in ml_df_balanced['Severity'].value_counts().sort_index().items():\n",
    "    pct = count / len(ml_df_balanced) * 100\n",
    "    bar = '‚ñà' * int(pct / 2)\n",
    "    print(f\"  Severity {sev}: {count:>10,} ({pct:>5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  Total samples: {len(ml_df_balanced):,}\")\n",
    "print(f\"  From original: {len(ml_df):,}\")\n",
    "print(f\"  Target per class: {TARGET_PER_CLASS:,}\")\n",
    "print(f\"  Perfect balance achieved: All classes at 25.0%\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 6: PREPARE DATA FOR MULTI-CLASS CLASSIFICATION\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 6] PREPARING MULTI-CLASS CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Separate features and target\n",
    "multiclass_X = ml_df_balanced.drop('Severity', axis=1)\n",
    "multiclass_y = ml_df_balanced['Severity']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "multiclass_y_encoded = label_encoder.fit_transform(multiclass_y)\n",
    "\n",
    "print(f\"\\nDataset prepared:\")\n",
    "print(f\"  Features: {multiclass_X.shape[1]}\")\n",
    "print(f\"  Samples: {len(multiclass_y_encoded):,}\")\n",
    "print(f\"  Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Train-test split (80/20) with stratification\n",
    "multiclass_X_train, multiclass_X_test, multiclass_y_train, multiclass_y_test = train_test_split(\n",
    "    multiclass_X, multiclass_y_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=multiclass_y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain-test split:\")\n",
    "print(f\"  Training: {len(multiclass_X_train):,} samples ({len(multiclass_X_train)/len(multiclass_X)*100:.0f}%)\")\n",
    "print(f\"  Testing:  {len(multiclass_X_test):,} samples ({len(multiclass_X_test)/len(multiclass_X)*100:.0f}%)\")\n",
    "\n",
    "# Scale features\n",
    "multiclass_scaler = StandardScaler()\n",
    "multiclass_X_train_scaled = multiclass_scaler.fit_transform(multiclass_X_train)\n",
    "multiclass_X_test_scaled = multiclass_scaler.transform(multiclass_X_test)\n",
    "\n",
    "# Store feature names\n",
    "multiclass_feature_names = multiclass_X.columns.tolist()\n",
    "\n",
    "print(\"‚úì Data scaled and ready for training\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 7: PREPARE DATA FOR BINARY CLASSIFICATION\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 7] PREPARING BINARY CLASSIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create binary target: Severity 1,2 ‚Üí LOW (0), Severity 3,4 ‚Üí HIGH (1)\n",
    "binary_y = ml_df_balanced['Severity'].map({1: 0, 2: 0, 3: 1, 4: 1})\n",
    "\n",
    "print(\"\\nBinary mapping:\")\n",
    "print(\"  Severity 1, 2 ‚Üí LOW  (0)\")\n",
    "print(\"  Severity 3, 4 ‚Üí HIGH (1)\")\n",
    "\n",
    "print(f\"\\nBinary distribution:\")\n",
    "for cls, count in binary_y.value_counts().sort_index().items():\n",
    "    label = \"LOW\" if cls == 0 else \"HIGH\"\n",
    "    pct = count / len(binary_y) * 100\n",
    "    print(f\"  {label:4} ({cls}): {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Train-test split for binary\n",
    "binary_X_train, binary_X_test, binary_y_train, binary_y_test = train_test_split(\n",
    "    multiclass_X, binary_y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=binary_y\n",
    ")\n",
    "\n",
    "print(f\"\\nBinary split:\")\n",
    "print(f\"  Training: {len(binary_X_train):,} samples\")\n",
    "print(f\"  Testing:  {len(binary_X_test):,} samples\")\n",
    "\n",
    "# Scale features for binary\n",
    "binary_scaler = StandardScaler()\n",
    "binary_X_train_scaled = binary_scaler.fit_transform(binary_X_train)\n",
    "binary_X_test_scaled = binary_scaler.transform(binary_X_test)\n",
    "\n",
    "# Store feature names\n",
    "binary_feature_names = binary_X_train.columns.tolist()\n",
    "\n",
    "print(\"‚úì Binary data scaled and ready\")\n",
    "\n",
    "# =============================================================================\n",
    "# PART 8: TRAIN MULTI-CLASS MODELS\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 8] TRAINING MULTI-CLASS MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "\n",
    "# Check for optional libraries\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except (ImportError, Exception):\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  XGBoost not available\")\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except (ImportError, Exception):\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è  LightGBM not available\")\n",
    "\n",
    "# Define models\n",
    "multiclass_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000, \n",
    "        random_state=42, \n",
    "        multi_class='multinomial',\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    multiclass_models['XGBoost'] = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='mlogloss'\n",
    "    )\n",
    "\n",
    "# Add LightGBM if available\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    multiclass_models['LightGBM'] = LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "# Train models\n",
    "multiclass_results = {}\n",
    "multiclass_predictions = {}\n",
    "multiclass_probabilities = {}\n",
    "multiclass_trained_models = {}\n",
    "\n",
    "print(f\"\\nTraining {len(multiclass_models)} models on {len(multiclass_X_train):,} samples...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, model in multiclass_models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"  \" + \"-\" * 40)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Determine if scaling is needed\n",
    "    needs_scaling = 'Logistic' in name or 'KNN' in name or 'Neural' in name\n",
    "    \n",
    "    # Train model\n",
    "    if needs_scaling:\n",
    "        model.fit(multiclass_X_train_scaled, multiclass_y_train)\n",
    "        y_pred = model.predict(multiclass_X_test_scaled)\n",
    "        y_prob = model.predict_proba(multiclass_X_test_scaled)\n",
    "    else:\n",
    "        model.fit(multiclass_X_train, multiclass_y_train)\n",
    "        y_pred = model.predict(multiclass_X_test)\n",
    "        y_prob = model.predict_proba(multiclass_X_test)\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Store model and predictions\n",
    "    multiclass_trained_models[name] = model\n",
    "    multiclass_predictions[name] = y_pred\n",
    "    multiclass_probabilities[name] = y_prob\n",
    "    \n",
    "    # Calculate metrics\n",
    "    multiclass_results[name] = {\n",
    "        'Accuracy': accuracy_score(multiclass_y_test, y_pred),\n",
    "        'Precision': precision_score(multiclass_y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'Recall': recall_score(multiclass_y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'F1-Score': f1_score(multiclass_y_test, y_pred, average='weighted', zero_division=0),\n",
    "        'Training Time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy:  {multiclass_results[name]['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {multiclass_results[name]['Precision']:.4f}\")\n",
    "    print(f\"  Recall:    {multiclass_results[name]['Recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {multiclass_results[name]['F1-Score']:.4f}\")\n",
    "    print(f\"  Time:      {train_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ MULTI-CLASS TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# PART 9: TRAIN BINARY MODELS\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 9] TRAINING BINARY CLASSIFICATION MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Define binary models\n",
    "binary_models = {\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Add XGBoost if available\n",
    "if XGBOOST_AVAILABLE:\n",
    "    binary_models['XGBoost'] = XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "\n",
    "# Add LightGBM if available\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    binary_models['LightGBM'] = LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        verbose=-1\n",
    "    )\n",
    "\n",
    "# Train binary models\n",
    "binary_results = {}\n",
    "binary_predictions = {}\n",
    "binary_probabilities = {}\n",
    "binary_trained_models = {}\n",
    "\n",
    "print(f\"\\nTraining {len(binary_models)} models...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, model in binary_models.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"  \" + \"-\" * 40)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Determine if scaling is needed\n",
    "    needs_scaling = 'Logistic' in name or 'KNN' in name or 'Naive' in name\n",
    "    \n",
    "    # Train model\n",
    "    if needs_scaling:\n",
    "        model.fit(binary_X_train_scaled, binary_y_train)\n",
    "        y_pred = model.predict(binary_X_test_scaled)\n",
    "        y_prob = model.predict_proba(binary_X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(binary_X_train, binary_y_train)\n",
    "        y_pred = model.predict(binary_X_test)\n",
    "        y_prob = model.predict_proba(binary_X_test)[:, 1]\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    # Store model and predictions\n",
    "    binary_trained_models[name] = model\n",
    "    binary_predictions[name] = y_pred\n",
    "    binary_probabilities[name] = y_prob\n",
    "    \n",
    "    # Calculate metrics\n",
    "    binary_results[name] = {\n",
    "        'Accuracy': accuracy_score(binary_y_test, y_pred),\n",
    "        'Precision': precision_score(binary_y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(binary_y_test, y_pred, zero_division=0),\n",
    "        'F1-Score': f1_score(binary_y_test, y_pred, zero_division=0),\n",
    "        'AUC-ROC': roc_auc_score(binary_y_test, y_prob),\n",
    "        'Training Time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy:  {binary_results[name]['Accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {binary_results[name]['Precision']:.4f}\")\n",
    "    print(f\"  Recall:    {binary_results[name]['Recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {binary_results[name]['F1-Score']:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {binary_results[name]['AUC-ROC']:.4f}\")\n",
    "    print(f\"  Time:      {train_time:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ BINARY CLASSIFICATION TRAINING COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =============================================================================\n",
    "# PART 10: SUMMARY & MODEL RANKINGS\n",
    "# =============================================================================\n",
    "print(\"\\n[PART 10] FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüèÜ MULTI-CLASS MODEL RANKINGS (by Accuracy):\")\n",
    "print(\"-\" * 80)\n",
    "sorted_multiclass = sorted(multiclass_results.items(), key=lambda x: x[1]['Accuracy'], reverse=True)\n",
    "for rank, (model, metrics) in enumerate(sorted_multiclass, 1):\n",
    "    emoji = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"  \"\n",
    "    print(f\"{emoji} {rank}. {model:25} ‚Üí Acc: {metrics['Accuracy']:.4f} | F1: {metrics['F1-Score']:.4f} | Time: {metrics['Training Time']:.1f}s\")\n",
    "\n",
    "print(\"\\nüèÜ BINARY MODEL RANKINGS (by AUC-ROC):\")\n",
    "print(\"-\" * 80)\n",
    "sorted_binary = sorted(binary_results.items(), key=lambda x: x[1]['AUC-ROC'], reverse=True)\n",
    "for rank, (model, metrics) in enumerate(sorted_binary, 1):\n",
    "    emoji = \"ü•á\" if rank == 1 else \"ü•à\" if rank == 2 else \"ü•â\" if rank == 3 else \"  \"\n",
    "    print(f\"{emoji} {rank}. {model:25} ‚Üí AUC: {metrics['AUC-ROC']:.4f} | F1: {metrics['F1-Score']:.4f} | Time: {metrics['Training Time']:.1f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ TRAINING COMPLETE - OPTIMIZED WITH 268K SAMPLES (67K per class)!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìà Key Improvements in this version:\")\n",
    "print(f\"  ‚úì Target: {TARGET_PER_CLASS:,} samples per class ({TARGET_PER_CLASS * 4:,} total)\")\n",
    "print(f\"  ‚úì Missing values imputed (not dropped) to preserve minority class samples\")\n",
    "print(f\"  ‚úì SMOTE used for synthetic oversampling of minority classes\")\n",
    "print(f\"  ‚úì Better representation of rare severity levels (1 and 4)\")\n",
    "print(f\"  ‚úì More robust model performance and generalization\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48406f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
